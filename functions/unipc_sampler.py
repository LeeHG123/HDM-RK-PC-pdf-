from typing import Optional,Tuple
import torch, tqdm
from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler
from functions.sde import VPSDE1D


def make_unipc_scheduler(
        sde: VPSDE1D,                    # â† í•„ìˆ˜
        num_train_steps: int = 1_000,
        device: str | torch.device = "cuda",
        *,                           # --- ì¶”ê°€ ---
        solver_order: int = 4,       #   2Â·3Â·4 ì§€ì›
        lower_order_final: bool = True,
) -> UniPCMultistepScheduler:
    """
    HDM VPSDE1Dì™€ ìˆ˜ì¹˜ì ìœ¼ë¡œ ë™ì¼í•œ Î±Ì„(t), Î²(t) ì´ì‚°í™” í…Œì´ë¸”ì„ ê°–ëŠ”
    UniPC-Multistep Schedulerë¥¼ ìƒì„±í•œë‹¤.
    ----------
    - t_i = (i+1)/N Â· T  (i=0,â€¦,Nâˆ’1)
    - Î±Ì„â‚€ = 1,  Î²_i = 1 âˆ’ Î±Ì„(t_i)/Î±Ì„(t_{iâˆ’1})
    """
    T = sde.T                              # 0.9946  (cosine schedule)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 0 â‰¤ tâ‚€ < â€¦ < t_N = T  (ê¸¸ì´ N+1)
    # Î²_i := 1 âˆ’ Î±Ì„(t_i)/Î±Ì„(t_{iâˆ’1})   (i = 1â€¦N)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ts = torch.linspace(
        0.0, T, num_train_steps + 1,         # shape (N+1,)
        dtype=torch.float64, device=device
    )

    log_alpha = sde.marginal_log_mean_coeff(ts)          # ln Î±Ì„(t)
    alpha_bar = torch.exp(log_alpha)                     # Î±Ì„(t)

    alpha_bar_t = alpha_bar[1:]
    alpha_bar_t_minus_1 = alpha_bar[:-1]
    
    # beta_t = 1 - (alpha_bar_t / alpha_bar_t-1)
    # ë¶„ëª¨ê°€ 0ì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³ , ë¹„ìœ¨ì´ 1ì„ ë„˜ì§€ ì•Šë„ë¡ clamp
    beta_t = 1.0 - torch.clamp(alpha_bar_t / alpha_bar_t_minus_1, max=0.9999)
    
    # ìµœì¢… betas ê°’ë„ ì•ˆì •ì ì¸ ë²”ìœ„ ë‚´ë¡œ í´ë¨í•‘
    betas = torch.clamp(beta_t, 0.0, 0.9999)

    # diffusers ëŠ” CPU tensor/numpy ë¥¼ ê¸°ëŒ€ â†’ .cpu()
    scheduler = UniPCMultistepScheduler(
        trained_betas       = betas.float().cpu(),
        num_train_timesteps = len(betas),   # = N
        solver_type         = "bh2",        # ODE ëª¨ë“œ
        solver_order        = solver_order, # â˜… 3 ë˜ëŠ” 4 ì°¨
        lower_order_final   = lower_order_final,
        prediction_type     = "epsilon",
        thresholding        = False,
        sample_max_value    = 1.0,
    )  
    return scheduler


@torch.no_grad()
def sample_probability_flow_ode(
        model,
        sde: VPSDE1D,
        batch_size: Optional[int] = None,
        data_dim: Optional[int] = None,
        device: torch.device = torch.device("cuda"),
        inference_steps: int = 500,
        fp16: bool = False,
        progress: bool = True,
        x_t0: Optional[torch.Tensor] = None,   
):
    """
    Args
    ----
    model            : score network,  (B, N) â†’ (B, N)
    sde              : VPSDE1D (ì‹œê°„ ìŠ¤ì¼€ì¼ T=0.9946 ê¸°ì¤€)
    inference_steps  : NFE (= scheduler steps)
    x_t0             : None ì´ë©´ N(0, I) ì—ì„œ ìƒ˜í”Œë§
    Returns
    -------
    x_0 (torch.Tensor) : (B, data_dim) ë³µì› ìƒ˜í”Œ
    """

    scheduler = make_unipc_scheduler(
        sde,
        num_train_steps=1000,
        device=device,
        solver_order=4,              # í•„ìš” ì‹œ 4ë¡œ ì¡°ì •
        lower_order_final=True,
    )
    scheduler.set_timesteps(inference_steps, device=device)

    # ì´ˆê¸°ë¶„í¬: p_T = ğ’©(0, Ïƒ_TÂ² I)
    if x_t0 is None:
        if batch_size is None or data_dim is None:
            raise ValueError("batch_size and data_dim must be provided if x_t0 is not given.")
        sigma_target = 1.0
        x = torch.randn(batch_size, data_dim, device=device) * sigma_target
    else:
        x = x_t0.to(device)
        # x_t0ì—ì„œ shape ì •ë³´ë¥¼ ì¶”ë¡ 
        batch_size, data_dim = x.shape

    model.eval()
    autocast = torch.cuda.amp.autocast if fp16 else torch.no_grad

    for i, t in enumerate(tqdm.tqdm(scheduler.timesteps, disable=not progress)):
        with autocast():
            #   scheduler.timesteps â†’ {N-1, â€¦, 0}
            #   ì‹¤ì œ ì‹œê°   táµ¢ = (i+1)/N Â· T  (cosine VPSDE)
            t_cont = t.to(torch.float32) / scheduler.config.num_train_timesteps * sde.T
            t_vec  = t_cont.repeat(batch_size).to(device).to(x.dtype)       # shape (B,)
            score  = model(x, t_vec)                       # score(x, t_cont)

        # diffusers expects epsilon â†’ Îµ = -Ïƒ_t Â· score
        sigma_t = scheduler.sigmas.to(device)[i]   
        epsilon = - score
        x = scheduler.step(epsilon, t, x).prev_sample

        flat  = x.view(x.size(0), -1)            # (B, N) ë¡œ í´ê¸°
        norm  = flat.norm(dim=1, keepdim=True)   # ê° ìƒ˜í”Œ L2-norm
        mask = (norm > 2.0).squeeze(1)          # Quadratic ì€ ì„ê³„ 10 Gaussianì€ 2
        flat[mask] = flat[mask] / norm[mask] * 2.0
        x = flat.view_as(x)

    return x

